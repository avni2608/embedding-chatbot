# -*- coding: utf-8 -*-
"""Copy_of_YT_Embeddings_LangChain (2) (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yW3ZoHDTCQnDihsAwIETh2As7whpA4-3

### Information Retreival - LangChain
- Without Using OpenAI Embeddings
- Without OpenAI LLM

Two Applications:
- Text Documents
- Multiple PDF Files
"""

!pip install langchain
!pip install huggingface_hub
!pip install sentence_transformers

"""### Get HUGGINGFACEHUB_API_KEY"""

import os
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

!pip install PyPDF2

"""### Download Text File"""

import requests

url = "https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt"
res = requests.get(url)
with open("state_of_the_union.txt", "w") as f:
  f.write(res.text)

# Document Loader
from langchain.document_loaders import TextLoader
loader = TextLoader('./Untitled.txt')
documents = loader.load()

documents

import textwrap

def wrap_text_preserve_newlines(text, width=110):
    # Split the input text into lines based on newline characters
    lines = text.split('\n')

    # Wrap each line individually
    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]

    # Join the wrapped lines back together using newline characters
    wrapped_text = '\n'.join(wrapped_lines)

    return wrapped_text

print(wrap_text_preserve_newlines(str(documents[0])))



# Text Splitter
from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=800)
docs = text_splitter.split_documents(documents)

len(docs)

docs[0]

!wget https://clusteriderrrrrrr.nyc3.cdn.digitaloceanspaces.com/data111.txt

"""### Embeddings"""

# Embeddings
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings()

!pip install faiss-cpu

# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html
from langchain.vectorstores import FAISS

db = FAISS.from_documents(docs, embeddings)

query = "what is article 17?"
docs = db.similarity_search(query)

print(wrap_text_preserve_newlines(str(docs[0].page_content)))

"""### Create QA Chain"""

from langchain.chains.question_answering import load_qa_chain
from langchain import HuggingFaceHub

llm=HuggingFaceHub(repo_id="google/flan-t5-xxl", model_kwargs={"temperature":0.3, "max_length":2000})

chain = load_qa_chain(llm, chain_type="stuff")

query = "what is The Constitution of India?"
docs = db.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = input("Enter the query")
docs = db.similarity_search(query)
chain.run(input_documents=docs, question=query, max_length=1000)

import random
GREET_INPUTS = ("hello","hi","greetings", "sup","what's up", "hey")
GREET_RESPONSES = ["hi", "hey", "*nods*","hi there", "hello", "i am glad! You are talking to me"]

def greet(sentence):
  for word in sentence.split():
    if word.lower() in GREET_INPUTS:
      return random.choice(GREET_RESPONSES)

flag = True
print("BOT: Hey i am on you can ask me anything")
while flag:
    user_response = input()
    print("ME:", user_response)
    if user_response != 'bye':
        if user_response == 'thanks' or user_response == 'thank you':
            flag = False
            print("BOT: THANKS BUDDY")
        else:
            if greet(user_response) is not None:
                print("BOT:", greet(user_response))
            else:
                query = user_response
                docs = db.similarity_search(query)
                output = chain.run(input_documents=docs, question=query)

                print("BOT: ",output)


    else:
        flag = False
        print("BOT: BYE BRO :(")

import requests
import os

def download_books(query):
    # Create a 'bookdata' directory if it doesn't exist
    if not os.path.exists('bookdata'):
        os.makedirs('bookdata')

    # Gutendex API search endpoint
    search_url = "https://gutendex.com/books"

    # Request search results for the query
    search_params = {"search": query}
    search_response = requests.get(search_url, params=search_params)

    # Check if the request is successful
    if search_response.status_code == 200:
        search_results = search_response.json()

        # Iterate through the search results and download the books
        for result in search_results["results"]:
            title = result["title"]
            print(f"Downloading {title}...")

            # Find the download link for the book
            for format in result["formats"]:
                if format["mime_type"] == "text/plain":
                    download_url = format["url"]

                    # Download the book and save it to the 'bookdata' directory
                    book_response = requests.get(download_url)
                    if book_response.status_code == 200:
                        with open(os.path.join('bookdata', f"{title}.txt"), "wb") as book_file:
                            book_file.write(book_response.content)
                            print(f"{title} downloaded successfully.")
                    else:
                        print(f"Error downloading {title}.")
                    break
    else:
        print("Error retrieving search results.")